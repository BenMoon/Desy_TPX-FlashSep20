{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load imports.py\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import param\n",
    "import yaml\n",
    "from holoviews import opts\n",
    "from tqdm import tqdm\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "from bokeh.io import export_png, export_svgs\n",
    "\n",
    "opts.defaults(\n",
    "    opts.Scatter(width=1000, height=300),\n",
    "    opts.Histogram(width=1000, height=300),\n",
    "    opts.Image(width=1000, height=300),\n",
    "    opts.Curve(width=1000, height=300),\n",
    "    opts.Points(width=1000, height=300),\n",
    ")\n",
    "\n",
    "\n",
    "%pylab inline\n",
    "# from matplotlib.colors import LogNorm\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (13.0, 6.0)\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def get_data_pd(fname: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        with h5py.File(fname, \"r\") as f:\n",
    "            rawNr = f[\"raw/trigger nr\"][:]\n",
    "            rawTof = f[\"raw/tof\"][:] * 1e6\n",
    "            rawTot = f[\"raw/tot\"][:]\n",
    "            rawX = f[\"raw/x\"][:]\n",
    "            rawY = f[\"raw/y\"][:]\n",
    "            centNr = f[\"centroided/trigger nr\"][:]\n",
    "            centTof = f[\"centroided/tof\"][:] * 1e6\n",
    "            centTot = f[\"centroided/tot max\"][:]\n",
    "            centY = f[\"centroided/y\"][:]\n",
    "            centX = f[\"centroided/x\"][:]\n",
    "\n",
    "        raw_data = pd.DataFrame(\n",
    "            np.column_stack((rawNr, rawTof, rawTot, rawX, rawY)),\n",
    "            columns=(\"nr\", \"tof\", \"tot\", \"x\", \"y\"),\n",
    "        )\n",
    "        cent_data = pd.DataFrame(\n",
    "            np.column_stack((centNr, centTof, centTot, centX, centY)),\n",
    "            columns=(\"nr\", \"tof\", \"tot\", \"x\", \"y\"),\n",
    "        )\n",
    "        return raw_data, cent_data\n",
    "    except:\n",
    "        print(f'key \"{keys}\" not known or file \"{fname}\" not existing')\n",
    "\n",
    "\n",
    "def gauss_fwhm(x, *p):\n",
    "    A, mu, fwhm = p\n",
    "    return A * np.exp(-((x - mu) ** 2) / (2.0 * (fwhm ** 2) / (4 * 2 * np.log(2))))\n",
    "\n",
    "\n",
    "def find_peaks_in_microbunch(\n",
    "    data: pd.DataFrame, nr_peaks: int = 4, dt: float = 10, offset: float = 0\n",
    ") -> list:\n",
    "    \"\"\"find first peak in micro-bunch\"\"\"\n",
    "    peaks = []\n",
    "    for i in range(nr_peaks):\n",
    "        mask = np.logical_and(\n",
    "            data[\"tof\"] > (offset + i * dt), data[\"tof\"] < (offset + i * dt + 1)\n",
    "        )\n",
    "        x_hist, x_edges = np.histogram(data[\"tof\"][mask], bins=1_000)\n",
    "        x = (x_edges[:-1] + x_edges[1:]) * 0.5\n",
    "        popt, pcov = curve_fit(\n",
    "            gauss_fwhm, x, x_hist, p0=[x_hist.max(), x[x_hist.argmax()], 0.05]\n",
    "        )\n",
    "        peaks.append(popt[1])\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def shift_microbunch_pulses(\n",
    "    data: pd.DataFrame, nr_peaks: int = 4, dt: float = 10, offset: float = 0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fold consecutive micro-bunch pulses back to first\"\"\"\n",
    "    peaks = find_peaks_in_microbunch(data, nr_peaks, dt, offset)\n",
    "\n",
    "    # shift bunches\n",
    "    for i in range(1, nr_peaks):\n",
    "        mask = np.logical_and(\n",
    "            data[\"tof\"] >= offset + i * dt, data[\"tof\"] < offset + (i + 1) * dt\n",
    "        )\n",
    "        data[\"tof\"][mask] -= peaks[i] - peaks[0]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "with open(\"runs.yaml\") as f:\n",
    "    runNrs = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = \"Spectral_r\"  # color map for velocity maps\n",
    "import matplotlib.colors as mplc\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.bag as db\n",
    "from dask.distributed import Client\n",
    "\n",
    "dask.config.set({\"distributed.dashboard.link\": \"/user-redirect/proxy/{port}/status\"})\n",
    "client = Client(n_workers=40, threads_per_worker=2)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance(mz: np.array, bins_mz: np.array) -> np.array:\n",
    "    \"\"\"compute 2D covariance map for every trigger event and sum them up\"\"\"\n",
    "    # mz = data[...,2]# for Melby: [...,0]*1e6\n",
    "    N = len(mz)\n",
    "    # bins_mz = np.linspace(0, 5, 2000)\n",
    "\n",
    "    # define necessary function for calculation dask\n",
    "    @dask.delayed\n",
    "    def calc_2d_hist(trigger_frames):\n",
    "        h_2b = np.zeros(2 * (len(bins_mz) - 1,))\n",
    "        for frame in trigger_frames:\n",
    "            # call this in dash client\n",
    "            h_1d_j = np.histogram(frame, bins=bins_mz)[0]\n",
    "            h_2b += h_1d_j[:, None] * h_1d_j[None, :]\n",
    "        return h_2b\n",
    "\n",
    "    @dask.delayed\n",
    "    def add_matrix(a, b):\n",
    "        return a + b\n",
    "\n",
    "    # calculate 2D histogram, covariance mape\n",
    "    output = []\n",
    "    for i in range(0, len(mz), 1024):\n",
    "        output.append(calc_2d_hist(mz[i : i + 1024]))\n",
    "    zs = dask.persist(*output)\n",
    "\n",
    "    # tree summation\n",
    "    empty = np.zeros(2 * (len(bins_mz) - 1,))\n",
    "    L = zs\n",
    "    while len(L) > 1:\n",
    "        new_L = []\n",
    "        for i in range(0, len(L), 2):\n",
    "            if i + 1 < len(L):\n",
    "                lazy = add_matrix(L[i], L[i + 1])\n",
    "            else:\n",
    "                lazy = add_matrix(L[i], empty)\n",
    "            new_L.append(lazy)\n",
    "        L = new_L\n",
    "\n",
    "    h_2b = dask.compute(L)[0][0]\n",
    "\n",
    "    return h_2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"out/ion-run_0016_20200903-2202.hdf5\"\n",
    "name = os.path.basename(file).rstrip(\".hdf5\")\n",
    "data_raw, data_cent = get_data_pd(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = find_peaks_in_microbunch(\n",
    "    data_cent,\n",
    "    nr_peaks=runNrs[name][\"pulses\"],\n",
    "    dt=1 / runNrs[name][\"rep\"] * 1e3,\n",
    "    offset=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.logical_and(data_cent[\"tof\"] > 0, data_cent[\"tof\"] < 20)\n",
    "x_hist, x_edges = np.histogram(data_cent[\"tof\"][mask], bins=1_000)\n",
    "a = hv.Histogram((x_hist, x_edges)).opts(\n",
    "    title=f'{name}: {runNrs[name][\"sample\"]}, {runNrs[name][\"pulses\"]}x{runNrs[name][\"rep\"]} kHz',\n",
    "    xlabel=\"TOF (µs)\",\n",
    "    width=1200,\n",
    ")\n",
    "b = [hv.VLine(i).opts(line_width=0.5) for i in peaks]\n",
    "a * b[0] * b[1] * b[2] * b[3] * b[4]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = '../FlashNov19/out/20191031-060000_CH3I_fullFEL-test_merged.hdf5'\n",
    "name = os.path.basename(file).rstrip('.hdf5')\n",
    "data_raw, data_cent = get_data_pd(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_shifted = shift_microbunch_pulses(data_cent, nr_peaks=runNrs[name][\"pulses\"], dt=1/runNrs[name][\"rep\"]*1e3, offset=0.9)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mask = np.logical_and(data_shifted['tof'] > 0, data_shifted['tof'] < 50)\n",
    "x_hist, x_edges = np.histogram(data_shifted['tof'][mask], bins=1_000)\n",
    "a = hv.Histogram((x_hist, x_edges)).opts(xlabel='TOF (µs)', width=1200)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://holoviews.org/reference/elements/matplotlib/ErrorBars.html\n",
    "# https://www.nuomiphp.com/eplan/en/29552.html\n",
    "peaks = find_peaks_in_microbunch(\n",
    "    data_cent,\n",
    "    nr_peaks=runNrs[name][\"pulses\"],\n",
    "    dt=1 / runNrs[name][\"rep\"] * 1e3,\n",
    "    offset=0.9,\n",
    ")\n",
    "p = np.polyfit(range(len(peaks)), peaks, deg=1)\n",
    "poly1d_fn = np.poly1d(p)\n",
    "x = range(len(peaks))\n",
    "y = peaks\n",
    "\n",
    "errors = np.column_stack((x, y, poly1d_fn(x) - y))\n",
    "hv.Curve((x, poly1d_fn(x))).opts(height=400) * hv.Scatter(errors).opts(\n",
    "    xlabel=\"Peak number\", ylabel=\"Peak pos (µs)\", color=\"red\"\n",
    ") * hv.ErrorBars(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_fit(lambda x, m, t: m * x + t, range(len(peaks)), peaks)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fname = file\n",
    "with h5py.File(fname, 'r+') as f:\n",
    "    # get the mask for the individual shots\n",
    "    print('get mask for every event')\n",
    "    #rawnr = f['raw/trigger nr'][:]\n",
    "    trigger_nr = f['raw/trigger nr'][:]\n",
    "    x, y = f['raw/x'][:], f['raw/y'][:]\n",
    "    tof, tot = f['raw/tof'][:] * 1e6, f['raw/tot'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_nr = data_cent[\"nr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def foo(i):\n",
    "    return np.where(trigger_nr == i)[0]\n",
    "\n",
    "\n",
    "futures = []\n",
    "nraxis = np.unique(trigger_nr)  # get valid event Nrs\n",
    "b = db.from_sequence(nraxis, npartitions=100)\n",
    "b = b.map(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time slices = b.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if number of events in slices is the same as in trigger events\n",
    "_, trigger_frame_len = np.unique(data_cent[\"nr\"], return_counts=True)\n",
    "x_hist, x_edges = np.histogram(trigger_frame_len, bins=100)\n",
    "a = hv.Histogram((x_edges[:-1], x_hist)).opts(\n",
    "    xlabel=\"Number of clusters per trigger event\"\n",
    ")\n",
    "assert (\n",
    "    np.sum(list(map(len, slices)) - trigger_frame_len) == 0\n",
    "), \"the two arrays do not have the same content\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = np.max(list(map(len, slices)))\n",
    "print(f\"longest mask: {maxlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice vz~0\n",
    "# original from CH3I_TOFs.ipynb\n",
    "\n",
    "from scipy.constants import physical_constants\n",
    "\n",
    "x_cent, y_cent, radius = 136, 140, 112\n",
    "# https://www.wolframalpha.com/input/?i=26.5713+mm%2F%2810+mm%2Fus+*+2.2567+us%29 from Benjamin Email\n",
    "vmi_magnification = 28.6445 / (10 * 2.4275)\n",
    "c_pixel = 78 / (2 * radius) * 1e-3  # 78mm / (224 pixel) conversion von pixel nach m\n",
    "# https://confluence.desy.de/display/TPX3BT/2020/12/17/VMI+calibration+with+Simion\n",
    "tof_offset = 0.926 - 0.12  # arrival time of photon peak\n",
    "\n",
    "df = data_cent.query(\"tof >= 3.1 & tof < 3.3\")\n",
    "df[\"tof\"] -= tof_offset\n",
    "df[\"x_rel\"] = df[\"x\"] - x_cent\n",
    "df[\"y_rel\"] = df[\"y\"] - y_cent\n",
    "df[\"r\"] = np.sqrt(df[\"x_rel\"] ** 2 + df[\"y_rel\"] ** 2)\n",
    "df[\"theta\"] = np.arctan2(\n",
    "    df[\"y_rel\"], df[\"x_rel\"]\n",
    ")  # alternatively: np.arctan2(df['y'], df['r'])\n",
    "df[\"v_x\"] = df[\"x_rel\"] * c_pixel / vmi_magnification / (df[\"tof\"] * 1e-6)  # m/s\n",
    "df[\"v_y\"] = df[\"y_rel\"] * c_pixel / vmi_magnification / (df[\"tof\"] * 1e-6)  # m/s\n",
    "df[\"xy_velocity\"] = (\n",
    "    df[\"r\"] * c_pixel / vmi_magnification / (df[\"tof\"] * 1e-6)\n",
    ")  # for m/s\n",
    "\n",
    "x_hist, x_edges = np.histogram(df[\"tof\"], bins=100)\n",
    "x = 0.5 * (x_edges[1:] + x_edges[:-1])\n",
    "popt, pcov = curve_fit(\n",
    "    gauss_fwhm, x, x_hist, p0=[x_hist.max(), x[x_hist.argmax()], 0.20]\n",
    ")\n",
    "\n",
    "e = physical_constants[\"elementary charge\"][0]  # C\n",
    "u = physical_constants[\"atomic mass constant\"][0]\n",
    "E = 269.5 * 100  # 158.9 * 100  # V/m, from Benjamins Simion simulation\n",
    "m = 14.0067 * u  # kg\n",
    "Δt = (df[\"tof\"] - popt[1]) * 1e-6  # convert to s\n",
    "df[\"vz\"] = Δt * e * E / m\n",
    "\n",
    "tof_t0 = popt[1] + tof_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δt = 15_000 * 1e6 * m / e / E  # calculate TOF in µs from vz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit first peak\n",
    "df = data_cent.query(\"tof >= 3.1 & tof < 3.3\")\n",
    "x_hist, x_edges = np.histogram(df[\"tof\"], bins=200)\n",
    "x = 0.5 * (x_edges[1:] + x_edges[:-1])\n",
    "popt, pcov = curve_fit(\n",
    "    gauss_fwhm, x, x_hist, p0=[x_hist.max(), x[x_hist.argmax()], 0.20]\n",
    ")\n",
    "\n",
    "x = 0.5 * (x_edges[1:] + x_edges[:-1])\n",
    "b = hv.Curve(\n",
    "    (x, gauss_fwhm(x, *popt)), label=f\"FWHM {1e3*popt[2]:.1f} ns, x₀={popt[1]:.3f} µs\"\n",
    ")\n",
    "hv.Histogram((x_hist, x_edges)).opts(\n",
    "    xlabel=\"TOF (µs)\", title=\"N₂ from 1st micro-bunch\"\n",
    ") * b * hv.VLine(tof_t0 - Δt) * hv.VLine(tof_t0 + Δt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if calculated offset is correct\n",
    "df = data_cent.query(\"tof >= 7.1 & tof < 7.33\")\n",
    "df[\"tof\"] -= peaks[1] - peaks[0]\n",
    "x_hist, x_edges = np.histogram(df[\"tof\"], bins=200)\n",
    "x = 0.5 * (x_edges[1:] + x_edges[:-1])\n",
    "popt, pcov = curve_fit(\n",
    "    gauss_fwhm, x, x_hist, p0=[x_hist.max(), x[x_hist.argmax()], 0.20]\n",
    ")\n",
    "\n",
    "x = 0.5 * (x_edges[1:] + x_edges[:-1])\n",
    "b = hv.Curve(\n",
    "    (x, gauss_fwhm(x, *popt)), label=f\"FWHM {1e3*popt[2]:.1f} ns, x0={popt[1]:.3f} µs\"\n",
    ")\n",
    "hv.Histogram((x_hist, x_edges)).opts(\n",
    "    xlabel=\"TOF (µs)\", title=\"N₂ from 2nd micro-bunch, backfolded\"\n",
    ") * b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# old serial code, next cell runs with Dask and is much faster\n",
    "\n",
    "offset = 0.9\n",
    "dt = 1 / runNrs[name][\"rep\"] * 1e3\n",
    "nr_peaks = runNrs[name][\"pulses\"]\n",
    "peaks = find_peaks_in_microbunch(data_cent, nr_peaks=nr_peaks, dt=dt, offset=offset)\n",
    "\n",
    "nr_events = len(slices)\n",
    "# initialize empty data array and fill with data\n",
    "data = np.zeros((nr_peaks * nr_events, maxlen, 3))\n",
    "\n",
    "# fill data array successive with data from each micro-bunch\n",
    "for i, mask in tqdm(enumerate(slices)):\n",
    "    mask = mask.tolist()\n",
    "    for peak_idx in range(0, nr_peaks):\n",
    "        # shift bunches\n",
    "        mask_peak = np.logical_and(\n",
    "            data_cent[\"tof\"][mask] >= offset + peak_idx * dt,\n",
    "            data_cent[\"tof\"][mask] <  offset + (peak_idx + 1) * dt,\n",
    "        )\n",
    "        delta = peaks[peak_idx] - peaks[0]\n",
    "\n",
    "        # mask_merged = np.logical_and(mask, mask_peak)\n",
    "        mask_length = mask_peak.sum()  # summation True to see how long the is\n",
    "        data[i + peak_idx * nr_events, :mask_length, 0] = data_cent[\"x\"][mask][\n",
    "            mask_peak\n",
    "        ]\n",
    "        data[i + peak_idx * nr_events, :mask_length, 1] = data_cent[\"y\"][mask][\n",
    "            mask_peak\n",
    "        ]\n",
    "        data[i + peak_idx * nr_events, :mask_length, 2] = (\n",
    "            data_cent[\"tof\"][mask][mask_peak] - delta\n",
    "        )\n",
    "        # data[i, :len(mask), 3] = data_cent['tot'][mask]#tot[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "offset = 0.9\n",
    "dt = 1 / runNrs[name][\"rep\"] * 1e3\n",
    "nr_peaks = runNrs[name][\"pulses\"]\n",
    "peaks = find_peaks_in_microbunch(data_cent, nr_peaks=nr_peaks, dt=dt, offset=offset)\n",
    "\n",
    "nr_events = len(slices)\n",
    "# initialize empty data array and fill with data\n",
    "data = np.zeros((nr_peaks * nr_events, maxlen, 3))\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def micro_pulse_into_data(peak_idx):\n",
    "    delta = peaks[peak_idx] - peaks[0]\n",
    "\n",
    "    data = np.zeros((len(slices), maxlen, 3))\n",
    "    for i, mask in tqdm(enumerate(slices)):\n",
    "        mask = mask.tolist()\n",
    "        # shift bunches for whole spectrum\n",
    "        mask_peak = np.logical_and(\n",
    "            data_cent[\"tof\"][mask] >= offset + peak_idx * dt,\n",
    "            data_cent[\"tof\"][mask] < offset + (peak_idx + 1) * dt,\n",
    "        )\n",
    "        # tof_t0 gauss fit x0\n",
    "        # shift peak\n",
    "        # mask_peak = np.logical_and(\n",
    "        #    data_cent[\"tof\"][mask] >= peaks[peak_idx] + tof_t0 - peaks[0] - Δt,\n",
    "        #    data_cent[\"tof\"][mask] <= peaks[peak_idx] + tof_t0 - peaks[0] + Δt,\n",
    "        # )\n",
    "        mask_length = mask_peak.sum()  # summation True to see how long the is\n",
    "        if mask_length > 0:\n",
    "            data[i, :mask_length, 0] = data_cent[\"x\"][mask][mask_peak]\n",
    "            data[i, :mask_length, 1] = data_cent[\"y\"][mask][mask_peak]\n",
    "            data[i, :mask_length, 2] = data_cent[\"tof\"][mask][mask_peak] - delta\n",
    "            # data[i, :len(mask), 3] = data_cent['tot'][mask]#tot[mask]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "output = []\n",
    "for peak_idx in range(0, nr_peaks):\n",
    "    output.append(micro_pulse_into_data(peak_idx))\n",
    "data_dask = dask.compute(dask.persist(*output))\n",
    "data_dask = np.concatenate(data_dask[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# initialize empty data array and fill with data\n",
    "data_old = np.zeros((len(slices), maxlen, 3))\n",
    "for i, mask in enumerate(slices):\n",
    "    mask = mask.tolist()\n",
    "    data_old[i, : len(mask), 0] = data_cent[\"x\"][mask]\n",
    "    data_old[i, : len(mask), 1] = data_cent[\"y\"][mask]\n",
    "    data_old[i, : len(mask), 2] = data_cent[\"tof\"][mask]\n",
    "    # data[i, :len(mask), 3] = data_cent['tot'][mask]#tot[mask]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Histogram(\n",
    "    np.histogram(\n",
    "        data_dask[..., 2][(data_dask[..., 2] > 0) & (data_dask[..., 2] < 15)],\n",
    "        bins=1_000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mz = data_dask[..., 2]  # data[:72007,:,2]\n",
    "bins_mz = np.linspace(1, 5, 5_000)\n",
    "h_2b = compute_covariance(mz, bins_mz).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(h_2b, norm=mplc.LogNorm())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Image(h_2b, bounds=(bins_mz[0], bins_mz[0], bins_mz[-1], bins_mz[-1])).opts(\n",
    "    colorbar=False, logz=True, clim=(0.1, None), cmap=\"afmhot_r\", width=600, height=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_1d = np.histogram(mz.ravel(), bins=bins_mz)[0] / len(data)\n",
    "h_2d = h_1d[:, None] * h_1d[None, :]\n",
    "j1d = np.arange(len(bins_mz) - 1)\n",
    "jx, jy = np.meshgrid(j1d, j1d, indexing=\"ij\")\n",
    "h_2d[jx >= jy] = 0\n",
    "h_2b[jx >= jy] = 0\n",
    "\n",
    "# subtract correlated from uncorrelated map:\n",
    "h_cov = h_2b / len(mz) - h_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_2b = 2 * [bins_mz[0], bins_mz[-1]]\n",
    "plt.imshow(\n",
    "    h_cov.T,\n",
    "    origin=\"lower\",\n",
    "    extent=ext_2b,\n",
    "    interpolation=\"nearest\",\n",
    "    norm=mplc.LogNorm(),\n",
    "    cmap=\"afmhot_r\",\n",
    ")\n",
    "plt.xlabel(\"TOF (us)\")\n",
    "plt.ylabel(\"TOF (us)\")\n",
    "# plt.title(f'raw un-centroided {fname}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 13))\n",
    "plt.imshow(\n",
    "    h_cov.T,\n",
    "    origin=\"lower\",\n",
    "    extent=ext_2b,\n",
    "    interpolation=\"nearest\",\n",
    "    norm=mplc.LogNorm(),\n",
    "    vmin=1e-6,\n",
    "    vmax=1e-0,\n",
    "    #           norm=mplc.SymLogNorm(linthresh=1e-3), vmin=-10, vmax=10,\n",
    "    cmap=\"afmhot_r\",\n",
    ")\n",
    "# plt.colorbar()\n",
    "plt.xlabel(\"TOF (us)\")\n",
    "plt.ylabel(\"TOF (us)\")\n",
    "# plt.ylim(3.1, 3.3)\n",
    "# plt.xlim(3.1, 3.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/PIPICO_N2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(\n",
    "    h_cov.T,\n",
    "    origin=\"lower\",\n",
    "    extent=ext_2b,\n",
    "    interpolation=\"nearest\",\n",
    "    norm=mplc.LogNorm(),\n",
    "    vmin=1e-6,\n",
    "    vmax=5e-1,\n",
    "    #           norm=mplc.SymLogNorm(linthresh=1e-3), vmin=-10, vmax=10,\n",
    "    cmap=\"afmhot_r\",\n",
    ")\n",
    "# plt.colorbar()\n",
    "# plt.xlim(2.2, 3.5)\n",
    "# plt.ylim(2.2, 3.5)\n",
    "plt.xlabel(\"TOF (us)\")\n",
    "plt.ylabel(\"TOF (us)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/PIPICO_N2_zoom_average.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://doi.org/10.1103/PhysRevA.94.013426](https://journals.aps.org/pra/article/10.1103/PhysRevA.94.013426/figures/2/medium)\n",
    " \n",
    "Photoion-photoion coincidence map showing the ion breakup channels of N2 from x-ray-pump–x-ray-probe measurements. The axes are the times of flight (TOFs) of the two ions that are detected in coincidence and a momentum conservation filter has been applied. The color intensity scale is normalized to 6720 maximum counts and the total counts is 49 680."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(\n",
    "    h_cov.T,\n",
    "    origin=\"lower\",\n",
    "    extent=ext_2b,\n",
    "    interpolation=\"nearest\",\n",
    "    norm=mplc.LogNorm(),\n",
    "    vmin=1e-6,\n",
    "    vmax=5e-1,\n",
    "    #           norm=mplc.SymLogNorm(linthresh=1e-3), vmin=-10, vmax=10,\n",
    "    cmap=\"afmhot_r\",\n",
    ")\n",
    "# plt.colorbar()\n",
    "# plt.xlim(1, 2)\n",
    "# plt.ylim(3, 4)\n",
    "plt.xlabel(\"TOF (us)\")\n",
    "plt.ylabel(\"TOF (us)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/PIPICO_N2_zoom_average.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(\n",
    "    h_cov.T,\n",
    "    origin=\"lower\",\n",
    "    extent=ext_2b,\n",
    "    interpolation=\"nearest\",\n",
    "    norm=mplc.LogNorm(),\n",
    "    vmin=1e-6,\n",
    "    vmax=5e-1,\n",
    "    #           norm=mplc.SymLogNorm(linthresh=1e-3), vmin=-10, vmax=10,\n",
    "    cmap=\"afmhot_r\",\n",
    ")\n",
    "# plt.colorbar()\n",
    "plt.xlim(3, 3.5)\n",
    "plt.ylim(3, 3.5)\n",
    "plt.xlabel(\"TOF (us)\")\n",
    "plt.ylabel(\"TOF (us)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
