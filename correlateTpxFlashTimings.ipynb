{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, concat, cut\n",
    "from datetime import datetime\n",
    "\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing_series(h5file: h5py.File, flash:int) -> Series:\n",
    "    assert flash in [1, 2], \"Precond.: FLASH 1 or 2\"\n",
    "    tGroup = h5file[f\"/FL{flash:1d}/Timing/Bunch pattern/train index {flash:1d}\"]\n",
    "    trainId = Series(tGroup[\"index\"], name=\"Train ID\")\n",
    "    return Series(tGroup[\"time\"], name=\"Date/Time\", index=trainId).apply(datetime.fromtimestamp)\n",
    "\n",
    "def dset_series(group: h5py.Group, pos=None) -> Series:\n",
    "    short_name = \"/\".join(group.name.split(\"/\")[-3:])\n",
    "    trainId = Series(group[\"index\"], name=\"Train ID\")\n",
    "    if pos:\n",
    "        return Series((group[\"value\"][i][pos] for i in trainId.index), name=short_name, index=trainId)\n",
    "    else:\n",
    "        return Series((group[\"value\"][i] for i in trainId.index), name=short_name, index=trainId)\n",
    "\n",
    "def h5_load(filename, dataset_dict):\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        time = timing_series(h5file, 1)       \n",
    "        dataset_list = [dset_series(h5file[value]) if type(value)==str \n",
    "                        else dset_series(h5file[value[0]],value[1]) for value in dataset_dict.values()]\n",
    "        df = concat([time]+ dataset_list, axis=1).dropna()\n",
    "        df.columns = ['time'] + [key for key in dataset_dict]\n",
    "        return df\n",
    "    \n",
    "def load_dataset(file_list, dataset_dict):\n",
    "    return dd.from_delayed([delayed(h5_load)(file, dataset_dict) for file in file_list])\n",
    "\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn\n",
    "\n",
    "def get_daq_files(id_min, id_max):\n",
    "    # create a database connection\n",
    "    conn = create_connection(database)\n",
    "    with conn:\n",
    "        sql = f'''SELECT name FROM files WHERE id IN \n",
    "            (SELECT file_id FROM trainIDs WHERE id >= {id_min} AND id <= {id_max})'''\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        daq_files = cur.fetchall()\n",
    "    if len(daq_files) > 0:\n",
    "        return [os.path.join('/asap3/fs-flash-o/gpfs/camp/2020/data/11010494/raw/hdf/express-0', i[0]) for i in daq_files]\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38626</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>80</li>\n",
       "  <li><b>Memory: </b>540.77 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:38626' processes=10 threads=80, memory=540.77 GB>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_cluster = Client() \n",
    "local_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get run-numbers\n",
    "#with open('runs.yaml', 'r') as f:\n",
    "#    runNrs = yaml.safe_load(f)\n",
    "#timepix_run_number = 865\n",
    "tpxFile = '/asap3/fs-flash-o/gpfs/camp/2020/data/11010494/processed/hdf5/e-run_0007_20200903-1517.hdf5'\n",
    "database = '/asap3/fs-flash-o/gpfs/camp/2020/data/11010494/processed/hdf5/trainIDs.db'\n",
    "\n",
    "ghz_adc_addr = '/FL1/Experiment/BL1/ADQ412 GHz ADC/CH00/TD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimePix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get trainIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainIDs(tpxFile):\n",
    "    with h5py.File(tpxFile, 'r') as f:\n",
    "        x2_trainIDs = f['timing/facility/train id'][:]\n",
    "        x2_timestamps = f['timing/facility/timestamp'][:]\n",
    "        tpx3_triggerNrs = f['timing/timepix/trigger nr'][:]\n",
    "        tpx3_timestamps = f['timing/timepix/timestamp'][:]\n",
    "    assert len(x2_trainIDs) == len(x2_timestamps), 'unmatching length'\n",
    "    assert len(tpx3_triggerNrs) == len(tpx3_timestamps), 'unmatching length'\n",
    "    assert len(set(x2_trainIDs)) == len(x2_trainIDs), 'found duplicates'\n",
    "    assert len(set(x2_timestamps)) == len(x2_timestamps), 'found duplicates'\n",
    "    assert len(set(tpx3_triggerNrs)) == len(tpx3_triggerNrs), 'found duplicates'\n",
    "    assert len(set(tpx3_timestamps)) == len(tpx3_timestamps), 'found duplicates'\n",
    "\n",
    "    start_index = np.abs(x2_timestamps - tpx3_timestamps[0]).argmin()\n",
    "    print(start_index)\n",
    "\n",
    "    #assert not (missing_elements(x2_trainIDs[start_index:])), 'list of trainIDs is not continuous'\n",
    "    trainIDs = [x2_trainIDs[start_index]]\n",
    "    trigger_Nrs = [tpx3_triggerNrs[0]]\n",
    "    skip = 1\n",
    "    for i in range(len(tpx3_triggerNrs) - 1):\n",
    "        if (tpx3_triggerNrs[i + 1] - tpx3_triggerNrs[i]) == 2:\n",
    "            skip += 1\n",
    "        try:\n",
    "            trainIDs.append(x2_trainIDs[start_index + i + skip])\n",
    "            trigger_Nrs.append(tpx3_triggerNrs[i+1])\n",
    "        except IndexError:\n",
    "            pass\n",
    "    assert len(trainIDs) == len(trigger_Nrs), 'matching fails'\n",
    "    \n",
    "    return (np.array(trigger_Nrs), np.array(trainIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3302 3302\n"
     ]
    }
   ],
   "source": [
    "triggers, trainIDs_tpx = get_trainIDs(tpxFile)\n",
    "print(len(triggers), len(trainIDs_tpx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(tpxFile, 'r') as f:\n",
    "    #tof = f['raw/tof'][:]\n",
    "    trigNr = f['raw/trigger nr'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , events = np.unique(trigNr, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLASH's ADC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_tof(trace, offset=10):\n",
    "    if np.sum(trace) == 0:\n",
    "        return -1\n",
    "    trace -= np.mean(trace)\n",
    "    return np.sum(trace[trace > offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.47 s, sys: 371 ms, total: 7.84 s\n",
      "Wall time: 7.77 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>ToF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 2 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                         time     ToF\n",
       "npartitions=1                        \n",
       "               datetime64[ns]  object\n",
       "                          ...     ...\n",
       "Dask Name: from-delayed, 2 tasks"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "id_min = int(trainIDs_tpx[0])\n",
    "id_max = int(trainIDs_tpx[-1])\n",
    "daq_files = get_daq_files(id_min, id_min+10)\n",
    "#adc_traces = np.float64(daq.valuesOfInterval(ghz_adc_addr, (tt1, tt2)))\n",
    "dataset_dict = {'ToF': '/FL1/Experiment/BL1/ADQ412 GHz ADC/CH00/TD'}\n",
    "\n",
    "df = load_dataset(daq_files, dataset_dict)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ToF',\n",
       " '_HTML_FMT',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_ufunc__',\n",
       " '__array_wrap__',\n",
       " '__await__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__dask_graph__',\n",
       " '__dask_keys__',\n",
       " '__dask_layers__',\n",
       " '__dask_optimize__',\n",
       " '__dask_postcompute__',\n",
       " '__dask_postpersist__',\n",
       " '__dask_scheduler__',\n",
       " '__dask_tokenize__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_args',\n",
       " '_bind_comparison_method',\n",
       " '_bind_operator',\n",
       " '_bind_operator_method',\n",
       " '_constructor',\n",
       " '_contains_index_name',\n",
       " '_cum_agg',\n",
       " '_describe_1d',\n",
       " '_describe_nonnumeric_1d',\n",
       " '_describe_numeric',\n",
       " '_elemwise',\n",
       " '_get_binary_operator',\n",
       " '_get_numeric_data',\n",
       " '_get_unary_operator',\n",
       " '_head',\n",
       " '_ipython_key_completions_',\n",
       " '_is_column_label_reference',\n",
       " '_is_index_level_reference',\n",
       " '_is_partition_type',\n",
       " '_meta',\n",
       " '_meta_nonempty',\n",
       " '_name',\n",
       " '_partition_type',\n",
       " '_partitions',\n",
       " '_reduction_agg',\n",
       " '_repr_data',\n",
       " '_repr_divisions',\n",
       " '_repr_html_',\n",
       " '_scalarfunc',\n",
       " '_select_columns_or_index',\n",
       " '_token_prefix',\n",
       " '_validate_axis',\n",
       " '_validate_chunks',\n",
       " '_var_1d',\n",
       " '_var_mixed',\n",
       " '_var_numeric',\n",
       " '_var_timedeltas',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'align',\n",
       " 'all',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply',\n",
       " 'applymap',\n",
       " 'assign',\n",
       " 'astype',\n",
       " 'bfill',\n",
       " 'categorize',\n",
       " 'clear_divisions',\n",
       " 'clip',\n",
       " 'clip_lower',\n",
       " 'clip_upper',\n",
       " 'columns',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compute',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'dask',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'divisions',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'eval',\n",
       " 'explode',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'first',\n",
       " 'floordiv',\n",
       " 'ge',\n",
       " 'get_dtype_counts',\n",
       " 'get_ftype_counts',\n",
       " 'get_partition',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'head',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'info',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'items',\n",
       " 'iterrows',\n",
       " 'itertuples',\n",
       " 'join',\n",
       " 'known_divisions',\n",
       " 'last',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lt',\n",
       " 'map_overlap',\n",
       " 'map_partitions',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'melt',\n",
       " 'memory_usage',\n",
       " 'memory_usage_per_partition',\n",
       " 'merge',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notnull',\n",
       " 'npartitions',\n",
       " 'nsmallest',\n",
       " 'nunique_approx',\n",
       " 'partitions',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'pivot_table',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'quantile',\n",
       " 'query',\n",
       " 'radd',\n",
       " 'random_split',\n",
       " 'rdiv',\n",
       " 'reduction',\n",
       " 'rename',\n",
       " 'repartition',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'select_dtypes',\n",
       " 'sem',\n",
       " 'set_index',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'shuffle',\n",
       " 'size',\n",
       " 'squeeze',\n",
       " 'std',\n",
       " 'sub',\n",
       " 'sum',\n",
       " 'tail',\n",
       " 'time',\n",
       " 'to_bag',\n",
       " 'to_csv',\n",
       " 'to_dask_array',\n",
       " 'to_delayed',\n",
       " 'to_hdf',\n",
       " 'to_html',\n",
       " 'to_json',\n",
       " 'to_parquet',\n",
       " 'to_records',\n",
       " 'to_sql',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'truediv',\n",
       " 'values',\n",
       " 'var',\n",
       " 'visualize',\n",
       " 'where']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_index() missing 1 required positional argument: 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-6919f5de0462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: set_index() missing 1 required positional argument: 'other'"
     ]
    }
   ],
   "source": [
    "df.set_index(inplace=True, sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tof_sum = []\n",
    "trainIDs_daq = []\n",
    "for i in range(len(adc_traces)):\n",
    "    trainIDs_daq.append(id_min + i)\n",
    "    tof_sum.append(sum_tof(adc_traces[i]))\n",
    "tof_sum = np.asarray(tof_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adc_traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, comm1, comm2 = np.intersect1d(trainIDs_daq, trainIDs_tpx-9, assume_unique=True, return_indices=True)\n",
    "tof_corr = tof_sum[comm1]\n",
    "tpx_corr = events[comm2]\n",
    "\n",
    "corr2, _ = spearmanr(tof_corr, tpx_corr)\n",
    "corr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tof_corr, tpx_corr, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "spear = []\n",
    "pear = []\n",
    "shift = []\n",
    "\n",
    "for s in range(-25,5):\n",
    "    val, comm1, comm2 = np.intersect1d(trainIDs_daq, trainIDs_tpx+s, assume_unique=True, return_indices=True)\n",
    "    tof_corr = tof_sum[comm1]\n",
    "    tpx_corr = events[comm2]\n",
    "        \n",
    "    tof_corr = np.array(tof_corr)\n",
    "    tpx_corr = np.array(tpx_corr)\n",
    "    corr, _  =  pearsonr(tof_corr, tpx_corr)\n",
    "    corr2, _ = spearmanr(tof_corr, tpx_corr)\n",
    "    pear.append(corr)\n",
    "    spear.append(corr2)\n",
    "    shift.append(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(shift, pear,'bo')\n",
    "plt.plot(shift, spear,'ro')\n",
    "plt.title(f' Pearson (blue) vs Spearman (red) ')\n",
    "plt.xlabel('shift')\n",
    "plt.ylabel('corr coef')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "spear = []\n",
    "pear = []\n",
    "shift = []\n",
    "\n",
    "for s in range(-25,5):\n",
    "\n",
    "    tof_corr = []\n",
    "    tpx_corr = []\n",
    " \n",
    "    for i in range(len(events)): # triggerNumbers TPX [0, 1, ..., 4332]\n",
    "        for j in range(len(trainIDs_daq)): # 619006674...\n",
    "            if (trainIDs_tpx[i]+s) == trainIDs_daq[j]:\n",
    "                tof_corr.append(tof_sum[j])\n",
    "                tpx_corr.append(events[i])\n",
    "\n",
    "    tof_corr = np.array(tof_corr)\n",
    "    tpx_corr = np.array(tpx_corr)\n",
    "    corr, _  =  pearsonr(tof_corr, tpx_corr)\n",
    "    corr2, _ = spearmanr(tof_corr, tpx_corr)\n",
    "    pear.append(corr)\n",
    "    spear.append(corr2)\n",
    "    shift.append(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(shift, pear,'bo')\n",
    "plt.plot(shift, spear,'ro')\n",
    "plt.title(f' Pearson (blue) vs Spearman (red) ')\n",
    "plt.xlabel('shift')\n",
    "plt.ylabel('corr coef')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check generated hdf5 file for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'out/run_0865_20191219-0926.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(file, 'r') as f:\n",
    "    #f['/timing/facility'].attrs['shift'] = -9\n",
    "    print(f['/timing/facility'].attrs['shift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda TimePix Env",
   "language": "python",
   "name": "timepix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
